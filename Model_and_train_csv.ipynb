{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saiteja-ms/Deepfake-detection-using-Deep-Learning-ResNet-and-LSTM-/blob/main/Model_and_train_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4571b22-9e19-476d-974e-34b997a47a7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4571b22-9e19-476d-974e-34b997a47a7b",
        "outputId": "712b65b6-27d5-46e5-90cc-cd86c3f14d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# before running this please change the RUNTIME to GPU (Runtime -> Change runtime type -> set hardware accelerator\n",
        "#Mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16cac07e-e660-4484-901b-79707277a872",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16cac07e-e660-4484-901b-79707277a872",
        "outputId": "70728193-5e41-4d0a-9499-5ba6400c119f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.7)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (19.24.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_recognition) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (9.4.0)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566170 sha256=0095255374986949f68a069620067141a10767c60f45428d034748938460f2cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face_recognition\n",
            "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "127076bc-64f3-4ec0-a4ad-837118df3c50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "127076bc-64f3-4ec0-a4ad-837118df3c50",
        "outputId": "5031e13a-09a4-4847-836c-6491327321ac"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error while calling cudaGetDevice(&the_device_id) in file /tmp/pip-install-plk_lhfd/dlib_511fe1a5b5ce41869787b18bc47695fd/dlib/cuda/gpu_data.cpp:204. code: 35, reason: CUDA driver version is insufficient for CUDA runtime version",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f2a86f762a82>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# Check if the file is corrupted or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/face_recognition/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1.2.3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_image_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_face_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_landmarks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare_faces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mcnn_face_detection_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_face_detector_model_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mcnn_face_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_face_detection_model_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_face_detection_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mface_recognition_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_recognition_model_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error while calling cudaGetDevice(&the_device_id) in file /tmp/pip-install-plk_lhfd/dlib_511fe1a5b5ce41869787b18bc47695fd/dlib/cuda/gpu_data.cpp:204. code: 35, reason: CUDA driver version is insufficient for CUDA runtime version"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import face_recognition\n",
        "# Check if the file is corrupted or not\n",
        "def validate_video(vid_path, train_transforms):\n",
        "    transform = train_transforms\n",
        "    count = 20\n",
        "    video_path = vid_path\n",
        "    frames = []\n",
        "    a = int(100/count)\n",
        "    first_frame = np.random.randint(0,a)\n",
        "    temp_video = video_path.split('/')[-1]\n",
        "    for i, frame in enumerate(frame_extract(video_path)):\n",
        "        frames.append(transform(frame))\n",
        "        if(len(frames) == count):\n",
        "            break\n",
        "    frames = torch.stack(frames)\n",
        "    frames = frames[:count]\n",
        "    return frames\n",
        "def frame_extract(path):\n",
        "    vidObj = cv2.VideoCapture(path)\n",
        "    success = 1\n",
        "    while success:\n",
        "        success, image = vidObj.read()\n",
        "        if success:\n",
        "            yield image\n",
        "\n",
        "im_size = 112\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                                        transforms.ToPILImage(),\n",
        "                                        transforms.Resize((im_size,im_size)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)])\n",
        "\n",
        "video_fil = glob.glob('/content/drive/MyDrive/Face_only_data/*.mp4')\n",
        "print(\"Total no of videos :\",len(video_fil))\n",
        "print(video_fil)\n",
        "count = 0\n",
        "for i in video_fil:\n",
        "    try:\n",
        "        count+=1\n",
        "        validate_video(i,train_transforms)\n",
        "    except:\n",
        "        print(\"Number of video processed: \",count,\" Remaining : \",(len(video_fil) - count))\n",
        "        print(\"Corrupted video is :\", i)\n",
        "        continue\n",
        "print((len(video_fil) - count))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import copy\n",
        "import random\n",
        "video_files =  glob.glob('/content/drive/MyDrive/Face_only_data/*.mp4')\n",
        "random.shuffle(video_files)\n",
        "random.shuffle(video_files)\n",
        "frame_count = []\n",
        "for video_file in video_files:\n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<100):\n",
        "    video_files.remove(video_file)\n",
        "    continue\n",
        "  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "print(\"frames are \" , frame_count)\n",
        "print(\"Total no of video: \" , len(frame_count))\n",
        "print('Average frame per video:',np.mean(frame_count))"
      ],
      "metadata": {
        "id": "-zyUl0uTYjwW"
      },
      "id": "-zyUl0uTYjwW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the JSON file\n",
        "json_file_path = '/path/to/your/json_file.json'\n",
        "\n",
        "# Directory where your extracted video files are stored\n",
        "video_dir = '/path/to/your/extracted_videos'\n",
        "\n",
        "# Output CSV file path\n",
        "output_csv_path = '/path/to/your/output.csv'\n",
        "\n",
        "# Load the JSON data\n",
        "with open(json_file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# List all video files in the video directory\n",
        "video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
        "\n",
        "# Initialize a list to hold the CSV data\n",
        "csv_data = []\n",
        "\n",
        "# Iterate over the video files and check their labels\n",
        "for video_file in video_files:\n",
        "    if video_file in data:\n",
        "        label = data[video_file]['label']\n",
        "        video_path = os.path.join(video_dir, video_file)\n",
        "        csv_data.append([video_path, label])\n",
        "    else:\n",
        "        print(f\"Video file {video_file} not found in JSON data\")\n",
        "\n",
        "# Create a DataFrame from the CSV data\n",
        "df = pd.DataFrame(csv_data, columns=['file', 'label'])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"CSV file saved to {output_csv_path}\")\n"
      ],
      "metadata": {
        "id": "A5mt8ZUb3Kca"
      },
      "id": "A5mt8ZUb3Kca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d02fc0-ac82-414a-ae9c-f5d1a3a4aa45",
      "metadata": {
        "id": "27d02fc0-ac82-414a-ae9c-f5d1a3a4aa45"
      },
      "outputs": [],
      "source": [
        "# load the video name and labels from csv\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27cd0a0d-bc70-4328-8d8b-5c06a729dd11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "27cd0a0d-bc70-4328-8d8b-5c06a729dd11",
        "outputId": "169c9694-6704-48a5-a2a8-b1c05057b12d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train :  18548\n",
            "test :  4638\n",
            "TRAIN:  Real 2999  FAKE: 15549\n",
            "TEST:  Real 532  FAKE: 4106\n",
            "Processing video: 000.mp4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Video file 000.mp4 not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1e10c8903176>\u001b[0m in \u001b[0;36m<cell line: 127>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0mvalid_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;31m# im_plot(image[0,:,:,:]) # Assuming im_plot is defined elsewhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1e10c8903176>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m#check if the video file exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Video file {video_path} not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Video file 000.mp4 not found"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "header_list = [\"file\", \"label\"]\n",
        "labels = pd.read_csv('', names=header_list)\n",
        "\n",
        "# Assuming video_files is defined somewhere in your code\n",
        "video_files = labels['file'].tolist()  # Ensure video_files is correctly populated\n",
        "\n",
        "train_videos = video_files[:int(0.8 * len(video_files))]\n",
        "valid_videos = video_files[int(0.8 * len(video_files)):]\n",
        "print(\"train : \", len(train_videos))\n",
        "print(\"test : \", len(valid_videos))\n",
        "\n",
        "# Define the function number_of_real_and_fake_videos\n",
        "def number_of_real_and_fake_videos(data_list):\n",
        "    header_list = [\"file\", \"label\"]\n",
        "    labels = pd.read_csv('', names=header_list)\n",
        "    fake = 0\n",
        "    real = 0\n",
        "    for i in data_list:\n",
        "        temp_video = i.split('/')[-1]\n",
        "        label_rows = labels.loc[labels[\"file\"] == temp_video]\n",
        "        if not label_rows.empty:\n",
        "            label = label_rows.iloc[0, 1]\n",
        "            if label == 'FAKE':\n",
        "                fake += 1\n",
        "            if label == 'REAL':\n",
        "                real += 1\n",
        "    return real, fake\n",
        "\n",
        "print(\"TRAIN: \", \"Real\", number_of_real_and_fake_videos(train_videos)[0], \" FAKE:\", number_of_real_and_fake_videos(train_videos)[1])\n",
        "print(\"TEST: \", \"Real\", number_of_real_and_fake_videos(valid_videos)[0], \" FAKE:\", number_of_real_and_fake_videos(valid_videos)[1])\n",
        "\n",
        "im_size = 112\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((im_size, im_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((im_size, im_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "class video_dataset(Dataset):\n",
        "    def __init__(self, video_names, labels, sequence_length=60, transform=None):\n",
        "        self.video_names = video_names\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.count = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_names[idx]\n",
        "        print(f\"Processing video: {video_path}\")\n",
        "\n",
        "        #check if the video file exists\n",
        "        if not os.path.exists(video_path):\n",
        "            raise FileNotFoundError(f\"Video file {video_path} not found\")\n",
        "\n",
        "        frames = []\n",
        "        a = int(100 / self.count)\n",
        "        first_frame = np.random.randint(0, a)\n",
        "        temp_video = video_path.split('/')[-1]\n",
        "        label_rows = self.labels.loc[self.labels['file'] == temp_video]\n",
        "        if not label_rows.empty:\n",
        "            label = label_rows.iloc[0, 1]\n",
        "            if label == 'FAKE':\n",
        "                label = 0\n",
        "            elif label == 'REAL':\n",
        "                label = 1\n",
        "            else:\n",
        "                raise ValueError(\"Unknown label value\")\n",
        "        else:\n",
        "            raise ValueError(f\"Label for video {video_path} not found\")\n",
        "\n",
        "        frame_count = 0\n",
        "        for i, frame in enumerate(self.frame_extract(video_path)):\n",
        "            frames.append(self.transform(frame))\n",
        "            frame_count += 1\n",
        "            if len(frames) == self.count:\n",
        "                break\n",
        "\n",
        "        if len(frames) == 0:\n",
        "            print(f\"No frames extracted for video: {video_path}\")\n",
        "            raise ValueError(f\"No frames extracted for video: {video_path}\")\n",
        "\n",
        "        print(f\"Extracted {frame_count} frames for video: {video_path}\")\n",
        "\n",
        "        frames = torch.stack(frames)\n",
        "        frames = frames[:self.count]\n",
        "        return frames, label\n",
        "\n",
        "    def frame_extract(self, path):\n",
        "        vidObj = cv2.VideoCapture(path)\n",
        "        success = True\n",
        "        while success:\n",
        "            success, image = vidObj.read()\n",
        "            if success and image is not None:\n",
        "                yield image\n",
        "            else:\n",
        "                break\n",
        "\n",
        "train_data = video_dataset(train_videos, labels, sequence_length=10, transform=train_transforms)\n",
        "val_data = video_dataset(valid_videos, labels, sequence_length=10, transform=test_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(val_data, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "image, label = train_data[0]\n",
        "# im_plot(image[0,:,:,:]) # Assuming im_plot is defined elsewhere\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a08d98af-cf88-4b4e-a1b5-b5081e0bfd7e",
      "metadata": {
        "id": "a08d98af-cf88-4b4e-a1b5-b5081e0bfd7e"
      },
      "outputs": [],
      "source": [
        "# Model with feature visualization\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes, latent_dim = 2048, lstm_layers = 1, hidden_dim = 2048, bidirectional = False):\n",
        "        super(Model, self).__init__()\n",
        "        model = models.resnext50_32x4d(pretrained = True) # Residual Network CNN\n",
        "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
        "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional=bidirectional)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.dp = nn.Dropout(0.4)\n",
        "        self.linear1 = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, num_classes, num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, c, h, w = x.shape\n",
        "        x = x.view(batch_size * seq_length, c, h, w)\n",
        "        fmap = self.model(x)\n",
        "        x = self.avgpool(fmap)\n",
        "        x = x.view(batch_size, seq_length, -1)\n",
        "        x_lstm, _ = self.lstm(x)\n",
        "        return fmap, self.dp(self.linear1(torch.mean(x_lstm, dim = 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4946190a-a8c0-48bd-b876-bf4895f02e60",
      "metadata": {
        "id": "4946190a-a8c0-48bd-b876-bf4895f02e60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e632d8cc-56b9-4c97-e974-57beecede027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Map Shape: torch.Size([20, 2048, 4, 4])\n",
            "Output Shape: torch.Size([1, 2])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the model\n",
        "model = Model(2).cuda()\n",
        "\n",
        "# Dummy input tensor\n",
        "input_tensor = torch.from_numpy(np.empty((1, 20, 3, 112, 112))).type(torch.cuda.FloatTensor)\n",
        "\n",
        "# Forward pass\n",
        "fmap, output = model(input_tensor)\n",
        "print(f\"Feature Map Shape: {fmap.shape}\")\n",
        "print(f\"Output Shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16c99a6e-b70b-4517-93ae-eec25593c2f6",
      "metadata": {
        "id": "16c99a6e-b70b-4517-93ae-eec25593c2f6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer):\n",
        "    model.train()\n",
        "    losses = AverageMeter()\n",
        "    t = []\n",
        "    for i, (inputs, targets) in enumerate(data_Loader):\n",
        "        if torch.cuda.is_available():\n",
        "            targets =  targets.type(torch.cuda.LongTensor)\n",
        "            inputs = inputs.cuda()\n",
        "        _, outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets.type(torch.cuda.LongTensor))\n",
        "        acc = calculate_accuracy(outputs, targets.type(torch.cuda.LongTensor))\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "        accuracies.update(acc, inputs.size(0))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        sys.stdout.write(\n",
        "                    \"\\r[Epoch %d/%d] [Batch %d / %d] [Loss: %f, Acc: %.2f%%]\"\n",
        "                    %(\n",
        "                        epoch,\n",
        "                        num_epochs,\n",
        "                        i,\n",
        "                        len(data_loader),\n",
        "                        losses.avg,\n",
        "                        accuracies.avg))\n",
        "    torch.save(model.state_dict(), '')\n",
        "    return losses.avg, accuracies.avg\n",
        "\n",
        "def test(epoch, model, data_loader, criterion):\n",
        "    print('Testing')\n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "    pred = []\n",
        "    true = []\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(data_loader):\n",
        "            if torch.cuda.is_available():\n",
        "                targets = targets.cuda().type(torch.cuda.FloatTensor)\n",
        "                inputs = inputs.cuda()\n",
        "            _, outputs = model(inputs)\n",
        "            loss = torch.mean(criterion(outputs, targets.type(torch.cuda.LongTensor)))\n",
        "            acc = calculate_accuracy(outputs, targets.type(torch.cuda.LongTensor))\n",
        "            _,p = torch.max(outputs, 1)\n",
        "            true += (targets.type(torch.cuda.LongTensor)).detach().cpu().numpy().reshape(len(targets)).tolist()\n",
        "            pred += p.detach().cpu().numpy().reshape(len(p)).tolist()\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            accuracies.update(acc, inputs.size(0))\n",
        "            sys.stdout.write(\n",
        "                    \"\\r[Batch %d / %d]  [Loss: %f, Acc: %.2f%%]\"\n",
        "                    %(\n",
        "                        i,\n",
        "                        len(data_loader),\n",
        "                        losses.avg,\n",
        "                        accuracies.avg\n",
        "                    )\n",
        "            )\n",
        "        print('\\nAccuracy {}'.format(accuracies.avg))\n",
        "    return true, pred, losses.avg, accuracies.avg\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val*n\n",
        "        self.count += n\n",
        "        self.avg = self.sum/self.count\n",
        "\n",
        "def calculate_accuracy(outputs, targets):\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = outputs.topk(1, 1, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(targets.view(1,-1))\n",
        "    n_correct_elems = correct.float().sum().item()\n",
        "    return 100*n_correct_elems / batch_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d2f59b5-8c60-4868-97a4-709823e4ec10",
      "metadata": {
        "id": "4d2f59b5-8c60-4868-97a4-709823e4ec10"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "#Output confusion matrix\n",
        "def print_confusion_matrix(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print('True positive = ', cm[0][0])\n",
        "    print('False positive = ', cm[0][1])\n",
        "    print('False negative = ', cm[1][0])\n",
        "    print('True negative = ', cm[1][1])\n",
        "    print('\\n')\n",
        "    df_cm = pd.DataFrame(cm, range(2), range(2))\n",
        "    sns.set(font_scale = 1.4) # for label size\n",
        "    sns.heatmap(df_cm, annot = True, annot_kws={\"size\":16})\n",
        "    plt.ylabel('Actual label',size=20)\n",
        "    plt.xlabel('Predicted label',size=20)\n",
        "    plt.xticks(np.arange(2),['Fake','Real'],size=16)\n",
        "    plt.yticks(np.arange(2), ['Fake', 'Real'], size=16)\n",
        "    plt.ylim([2,0])\n",
        "    plt.show()\n",
        "    calc_acc = (cm[0][0] + cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\n",
        "    print(\"Calculated Accuracy\", calc_acc*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b992650-79fa-4e39-8152-a196921dd34b",
      "metadata": {
        "id": "5b992650-79fa-4e39-8152-a196921dd34b"
      },
      "outputs": [],
      "source": [
        "def plot_loss(train_loss_avg, test_loss_avg, num_epochs):\n",
        "    loss_train = train_loss_avg\n",
        "    loss_val = test_loss_avg\n",
        "    print(num_epochs)\n",
        "    epochs = range(1, num_epochs+1)\n",
        "    plt.plot(epochs, loss_train, 'g', label='Training_loss')\n",
        "    plt.plot(epochs, loss_val, 'b', label='validation_loss')\n",
        "    plt.title('Training and Validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracy(train_accuracy, test_accuracy, num_epochs):\n",
        "    loss_train = train_accuracy\n",
        "    loss_val = test_accuracy\n",
        "    epochs = range(1, num_epochs+1)\n",
        "    plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
        "    plt.plot(epcohs, loss_val, 'b', label='Validation accuracy')\n",
        "    plt.title('Training and Validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c56dbe-a322-4379-8c62-2d369b91a88b",
      "metadata": {
        "id": "c7c56dbe-a322-4379-8c62-2d369b91a88b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "3031f5f9-eec4-4531-89ac-2fdead9ee028"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data_Loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-afb469050470>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtrain_loss_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-b16a6981d995>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, num_epochs, data_loader, model, criterion, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_Loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_Loader' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "#learning rate\n",
        "lr = 1e-5\n",
        "#number of epochs\n",
        "num_epochs = 20\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "train_loss_avg = []\n",
        "train_accuracy = []\n",
        "test_loss_avg = []\n",
        "test_accuracy = []\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    l, acc = train_epoch(epoch, num_epochs, train_loader, model, criterion, optimizer)\n",
        "    train_loss_avg.append(l)\n",
        "    train_accuracy.append(acc)\n",
        "    true, pred, tl, t_acc = test(epoch, model, valid_loader, criterion)\n",
        "    test_loss_avg.append(tl)\n",
        "    test_accuracy.append(t_acc)\n",
        "plot_loss(train_loss_avg, test_loss_avg, len(train_loss_avg))\n",
        "plot_accuracy(train_accuracy, test_accuracy, len(train_accuracy))\n",
        "print(confusion_matrix(true, pred))\n",
        "print_confusion_matrix(true, pred)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}